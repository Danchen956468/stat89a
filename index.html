<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>matrices-connector by dsten</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">STAT 89A Introduction to Matrices and Graphs in Data Science</h1>
      <h2 class="project-tagline">A Spring 2016 Data Science Connector Course </h2>
      <h3>CCN: 87302</h3>
      <h3>Michael Mahoney</h3>
      <h3>Monday 1:00-3:00 PM</h3>
      <h3>TBA</h3>
      <h3>Units: 2</h3>
      <a href="http://data8.org/" class="btn">Home</a>
      <a href="http://databears.berkeley.edu/" class="btn">Databears</a>
    </section>

    <section class="main-content">
      <h3>Welcome to STAT 89A Introduction to Matrices and Graphs in Data Science</h3>
 
<p>This connector will cover introductory topics in the mathematics of data science, focusing on discrete probability and linear algebra and the connections between them that are useful in modern theory and practice. We will focus on matrices and graphs as popular mathematical structures with which to model data. For examples, as models for term-document corpora, high-dimensional regression problems, ranking/classification of web data, adjacency properties of social network data, etc.</p>

<p>This course connects to the Foundations of Data Science course y providing a unified view of the mathematical methods underlie the theoretical foundations of data science. Typically, they are taught from a statistical perspective, or they are taught from a computer science perspective, or they are taught from the perspective that mathematics departments adopt. This connector course will try to drill down in more detail on several key ideas that are practically useful and have a rich mathematical underpinnings.</p>

<p>The course will cover some basic mathematics of discrete probability and linear algebra as well as ways they interact in data problems. The choice of discrete rather continuous probability is since it is what is used in practice and since many of the basic results can be illustrated at the freshman level without advanced calculus, etc. Basic insights are developed without getting bogged down into details that matter in traditional numerical presentations of linear algebra but that matter less for data science. To do this, we will focus on quadratic forms, rather than the subspace structure, of the linear algebra. Latter topics that use both discrete probability and linear algebra include simple geometric properties of high-dimensional spaces, and spectral methods for clustering and ranking, all of which have interesting mathematics and are widely-used.</p>

<p>Here is a tentative week-by-week outline.</p>

<h4>Week 1</h4>
<p>Introduction, motivation, and overview; representing data as flat tables versus matrices and graphs; different ways probability/randomness/noise is used with data, e.g., randomness in the data versus randomness in the algorithm; probability and matrices/graphs in data science versus other areas</p>

<h4>Week 2</h4>
<p>Random events and random variables; discrete versus continuous probability; union bound; mean, variance, etc.</p>

<h4>Week 3</h4>
<p>Independent versus uncorrelated; Markov, Chebychev.</p>

<h4>Week 4</h4>
<p>Chernoff-type inequalities.</p>

<h4>Week 5</h4>
<p>Coupon collector problems and the birthday paradox.</p>

<h4>Week 6</h4>
<p>Simple random walks.</p>

<h4>Week 7</h4>
<p>Vector spaces, subspaces, and orthogonality.</p>

<h4>Week 8</h4>
<p>Eigenvalues and eigenvectors, symmetric matrices; SVD versus eigendecomposition.</p>

<h4>Week 9</h4>
<p>Matrices as data versus as transformations; quadratic extremal properties of eigenvalues; connections between eigenvalues, subspaces, quadratic forms, and random walks.</p>

<h4>Week 10</h4>
<p>Peculiar properties of high-dimensional vector spaces (sphere versus cube, volume versus surface area).</p>

<h4>Week 11</h4>
<p>Peculiar properties, cont. (random projections and meaningfulness/non-meaningfulness of distances).</p>

<h4>Week 12</h4>
<p>Random graph models, including geometric random graphs, Erdos-Renyi, and small-world models.</p>

<h4>Week 13</h4>
<p>Random graph models, simple random walks and PageRank random walks on graphs (including diffusion versus quadratic optimization perspective, and eigenvalue versus linear equation perspective).</p>

<h4>Week 14</h4>
<p>Complementary uses of simple random walk spectral graph methods in classification, clustering, and ranking.</p>

<h4>Week 15</h4>
<p>Review, catch up if we got behind, and projects.</p>

<p>Homework will be a combination of pencil-and-paper and simulations/computations as well as part of a midterm/final project in lieu of exams. It builds on a rich set of examples and publicly-available real and synthetic data sets to illustrate points, and the simulation/computation homework/projects will use these.</p>
  </body>
  
<p>For more information on the Data Science Education Program at UC Berkeley, please visit databears.berkeley.edu</p>
</html>
